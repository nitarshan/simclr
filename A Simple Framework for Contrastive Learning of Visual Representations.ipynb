{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Simple Framework for Contrastive Learning of Visual Representations",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-RO4QG8EuvB",
        "colab_type": "text"
      },
      "source": [
        "# A Simple Framework for Contrastive Learning of Visual Representations\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nitarshan/simclr/blob/master/A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations.ipynb)\n",
        "\n",
        "Reference: [https://arxiv.org/abs/2002.05709](https://arxiv.org/abs/2002.05709)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTfVn613L_sk",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qshqfJESJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use wandb for logging\n",
        "!pip install --upgrade wandb --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPvByEJMBIF",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZV98T_rEYXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataclasses import asdict, dataclass\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from PIL.Image import Image\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "from torchvision.models import resnet18, resnet34, resnet50\n",
        "from torchvision.transforms import (\n",
        "  ColorJitter, Compose, Lambda, Normalize, RandomApply, RandomGrayscale,\n",
        "  RandomHorizontalFlip, RandomResizedCrop, ToTensor)\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCTQo7wJMCnN",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5EcciTUEZAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass(eq=True, frozen=True)\n",
        "class HParams:\n",
        "  cifar: int = 10\n",
        "  crop_size: int = 32\n",
        "  colour_distortion: int = 0.5\n",
        "  batch_size: int = 1024 # 256, 512, 1024, 2048, 4096 evaluated in paper\n",
        "  xent_temp: float = 0.5 # 0.1, 0.5, 1.0 evaluated in paper\n",
        "  proj_dim: int = 128\n",
        "  weight_decay: float = 1e-6\n",
        "  max_lr: float = 1.5 # 0.5, 1.0, 1.5 evaluated in paper\n",
        "  warmup_epochs: int = 10\n",
        "  cooldown_epochs: int = 90 # 90, 190, 290, 390, 490, 590, 690, 790, 890, 990 evaluated in paper\n",
        "  use_cosine_scheduler: bool = True\n",
        "  resnet_depth: int = 18 # 50 evaluated in paper\n",
        "  DEPTH_TO_REPR_DIM = {18: 512, 34: 512, 50: 2048}\n",
        "\n",
        "  def __post_init__(self):\n",
        "    assert self.cifar in (10, 100)\n",
        "    assert self.resnet_depth in self.DEPTH_TO_REPR_DIM\n",
        "  \n",
        "  @property\n",
        "  def repr_dim(self) -> int:\n",
        "    return self.DEPTH_TO_REPR_DIM[self.resnet_depth]\n",
        "\n",
        "  @property\n",
        "  def md5(self):\n",
        "    return hashlib.md5(str(hash(self)).encode('utf-8')).hexdigest()\n",
        "\n",
        "hp = HParams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo5Nnu3CMFtM",
        "colab_type": "text"
      },
      "source": [
        "## Image Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJzzBNCOEanZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimCLRAugment(object):\n",
        "  def __init__(self, hp: HParams):\n",
        "    s = hp.colour_distortion\n",
        "    self.simclr_augment = Compose([\n",
        "      RandomResizedCrop(hp.crop_size),\n",
        "      RandomHorizontalFlip(),\n",
        "      RandomApply([\n",
        "        ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "      ], p=0.8),\n",
        "      RandomGrayscale(p=0.2),\n",
        "    ])\n",
        "\n",
        "  def __call__(self, img: Image):\n",
        "    aug = self.simclr_augment(img)\n",
        "    return (img, aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q78wi2PMJRN",
        "colab_type": "text"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtb9uWk5EckD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loaders(hp: HParams):\n",
        "  if hp.cifar == 10:\n",
        "    dataset = CIFAR10\n",
        "  elif hp.cifar == 100:\n",
        "    dataset = CIFAR100\n",
        "\n",
        "  train_transform = Compose([\n",
        "    SimCLRAugment(hp),\n",
        "    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])),\n",
        "  ])\n",
        "  # Crops x Channels x Height x Width\n",
        "  train_dataset = dataset(\"./data\", train=True, transform=train_transform, download=True)\n",
        "\n",
        "  test_transform = ToTensor()\n",
        "  # Channels x Height x Width\n",
        "  test_dataset = dataset(\"./data\", train=False, transform=test_transform, download=True)\n",
        "\n",
        "  kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
        "  train_loader = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True, **kwargs)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=hp.batch_size, shuffle=False, **kwargs)\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVGPL0NMLFP",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and Projector Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KENee5c7Ed5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, hp: HParams):\n",
        "    super().__init__()\n",
        "    if hp.resnet_depth == 18:\n",
        "      self.resnet = resnet18(pretrained=False, num_classes=hp.cifar)\n",
        "    elif hp.resnet_depth == 34:\n",
        "      self.resnet = resnet34(pretrained=False, num_classes=hp.cifar)\n",
        "    elif hp.resnet_depth == 50:\n",
        "      self.resnet = resnet50(pretrained=False, num_classes=hp.cifar)\n",
        "    self.resnet.conv1 = torch.nn.Conv2d(\n",
        "      3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "    )\n",
        "    self.resnet.maxpool = torch.nn.Identity()\n",
        "    self.resnet.fc = torch.nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.resnet(x)\n",
        "\n",
        "class Projector(torch.nn.Module):\n",
        "  def __init__(self, hp: HParams):\n",
        "    super().__init__()\n",
        "    self.l1 = torch.nn.Linear(hp.repr_dim, hp.repr_dim)\n",
        "    self.l2 = torch.nn.Linear(hp.repr_dim, hp.proj_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.l1(x))\n",
        "    return self.l2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgt0WXnLMNkt",
        "colab_type": "text"
      },
      "source": [
        "## Contrastive Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebQR9TTIEfcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nt_xent_loss(z: torch.Tensor, xent_temp: float):\n",
        "  # z: (N x 2) x projection_dim\n",
        "  N = z.shape[0] // 2 # Can be less than batch_size for last batch in epoch\n",
        "  znorm = z / torch.norm(z, 2, dim=1, keepdim=True)\n",
        "  cos_sim = torch.einsum('id,jd->ij', znorm, znorm) / xent_temp\n",
        "  cos_sim.fill_diagonal_(-1e5)\n",
        "  l = -F.log_softmax(cos_sim, 1)\n",
        "  idxs = np.arange(N)\n",
        "  return (l[2*idxs,2*idxs+1] + l[2*idxs+1,2*idxs]).sum() / (2*N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ9C4JtzMSfy",
        "colab_type": "text"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcfrwBH7EiTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(encoder, projector, train_loader, optimizer, epoch, xent_temp: float) -> None:\n",
        "  encoder.train()\n",
        "  projector.train()\n",
        "\n",
        "  batch_losses = []\n",
        "\n",
        "  for data, target in tqdm(train_loader, leave=False, desc=f'epoch {epoch}'):\n",
        "    bs, ncrops, c, h, w = data.size()\n",
        "    data = data.cuda()\n",
        "    target = target.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    z = projector(encoder(data.view((-1,c,h,w))))\n",
        "    loss = nt_xent_loss(z, xent_temp)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_losses.append(loss.item())\n",
        "\n",
        "  wandb.log({\"Train Loss\": np.mean(batch_losses)}, step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yrcGS_8MTvj",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB2PbcqwEkEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def prepare_xy(encoder, loader):\n",
        "  encoder.eval()\n",
        "  projector.eval()\n",
        "\n",
        "  embeddings = []\n",
        "  for data, target in loader:\n",
        "    data = data.cuda()\n",
        "    if len(data.shape) == 5:\n",
        "      h = encoder(data[:,0,:,:,:])\n",
        "    else:\n",
        "      h = encoder(data)\n",
        "    embeddings.append((h.cpu().numpy(), target.numpy()))\n",
        "\n",
        "  X = np.concatenate([x[0] for x in embeddings])\n",
        "  y = np.concatenate([x[1] for x in embeddings])\n",
        "  return X, y\n",
        "\n",
        "def evaluate_logistic(X, y, Xt, yt, epoch: int) -> None:\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(X)\n",
        "  Xt = scaler.transform(Xt)\n",
        "\n",
        "  clf = LogisticRegression(\n",
        "    random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=1,\n",
        "  ).fit(X, y)\n",
        "  \n",
        "  results = {\n",
        "    'Train Evaluation': np.mean(clf.predict(X) == y),\n",
        "    'Test Evaluation': np.mean(clf.predict(Xt) == yt),\n",
        "  }\n",
        "  wandb.log(results, step=epoch)\n",
        "\n",
        "def evaluate_features(encoder, projector, train_loader, test_loader, epoch: int) -> None:\n",
        "  X, y = prepare_xy(encoder, train_loader)\n",
        "  Xt, yt = prepare_xy(encoder, test_loader)\n",
        "  evaluate_logistic(X, y, Xt, yt, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G96BQpgAMVmC",
        "colab_type": "text"
      },
      "source": [
        "## Restore State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbJjiEDFEkrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_state(hp, epoch: int, encoder, projector, optimizer, scheduler):\n",
        "  torch.save({\n",
        "    'hparams': hp,\n",
        "    'epoch': epoch,\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'projector_state_dict': projector.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "  }, hp.md5 + '.pkl')\n",
        "\n",
        "def load_state(hp):\n",
        "  try:\n",
        "    checkpoint = torch.load(hp.md5 + '.pkl')\n",
        "    return checkpoint\n",
        "  except FileNotFoundError:\n",
        "    return None\n",
        "\n",
        "checkpoint = load_state(hp)\n",
        "if checkpoint is not None:\n",
        "  print(f\"Restoring training state from epoch {checkpoint['epoch']}\")\n",
        "  hp = checkpoint['hparams']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdzWSAhcMY7p",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuCUK40LEmBI",
        "colab_type": "code",
        "outputId": "8f1c0ce3-f7ab-47ff-edd0-e9efc6270fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "torch.manual_seed(hash(hp))\n",
        "\n",
        "# Dataset\n",
        "train_loader, test_loader = get_loaders(hp)\n",
        "\n",
        "# Models\n",
        "encoder = Encoder(hp)\n",
        "projector = Projector(hp)\n",
        "if checkpoint is not None:\n",
        "  encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "  projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "encoder = encoder.cuda()\n",
        "projector = projector.cuda()\n",
        "\n",
        "# Optimizers and Schedulers\n",
        "init_lr = hp.max_lr / hp.warmup_epochs\n",
        "optimizer = SGD(chain(encoder.parameters(), projector.parameters()), lr=init_lr, weight_decay=1e-6)\n",
        "cosine_scheduler = CosineAnnealingLR(optimizer, hp.cooldown_epochs)\n",
        "if checkpoint is not None:\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  cosine_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "# Starting Epoch\n",
        "epoch = 1 if checkpoint is None else checkpoint['epoch']\n",
        "\n",
        "# Wandb\n",
        "wandb.init(anonymous='must')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/anony-moose-23573/uncategorized?apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9\" target=\"_blank\">https://app.wandb.ai/anony-moose-23573/uncategorized?apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/anony-moose-23573/uncategorized/runs/24q23skj?apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9\" target=\"_blank\">https://app.wandb.ai/anony-moose-23573/uncategorized/runs/24q23skj?apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/anony-moose-23573/uncategorized/runs/24q23skj?apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RcGbVmVMaxg",
        "colab_type": "text"
      },
      "source": [
        "## Train and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2oocJHJEniR",
        "colab_type": "code",
        "outputId": "67159b1c-db5f-4aee-cbb5-2e33689e586c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "%%wandb\n",
        "for epoch in range(epoch, hp.warmup_epochs + hp.cooldown_epochs + 1):\n",
        "  wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']}, step=epoch)\n",
        "  train(encoder, projector, train_loader, optimizer, epoch, hp.xent_temp)\n",
        "  \n",
        "  if epoch <= hp.warmup_epochs:\n",
        "    optimizer.param_groups[0]['lr'] = min(hp.max_lr, hp.max_lr * (epoch+1)/10) # Pytorch LambdaLR scheduler is buggy...\n",
        "  elif hp.use_cosine_scheduler:\n",
        "    cosine_scheduler.step()\n",
        "  \n",
        "  if (epoch == 1) or (epoch % 10 == 0) or (epoch == hp.warmup_epochs + hp.cooldown_epochs):\n",
        "    evaluate_features(encoder, projector, train_loader, test_loader, epoch)\n",
        "    save_state(hp, epoch + 1, encoder, projector, optimizer, cosine_scheduler)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://app.wandb.ai/anony-moose-23573/uncategorized/runs/24q23skj?jupyter=true&state=paused&apiKey=69fa44b48e12f5dbb395cb1e1dd77e94c1fde5d9\" style=\"border:none;width:100%;height:420px\">\n",
              "                </iframe>"
            ],
            "text/plain": [
              "<wandb.jupyter.Run at 0x7fb9b3cd7828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_be6HoGWVgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}